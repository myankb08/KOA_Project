{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13447353,"sourceType":"datasetVersion","datasetId":8535757}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport os\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport itertools","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T12:34:44.665280Z","iopub.execute_input":"2025-11-24T12:34:44.665953Z","iopub.status.idle":"2025-11-24T12:34:47.896516Z","shell.execute_reply.started":"2025-11-24T12:34:44.665932Z","shell.execute_reply":"2025-11-24T12:34:47.895596Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T12:34:50.839225Z","iopub.execute_input":"2025-11-24T12:34:50.840042Z","iopub.status.idle":"2025-11-24T12:34:50.877565Z","shell.execute_reply.started":"2025-11-24T12:34:50.840013Z","shell.execute_reply":"2025-11-24T12:34:50.876692Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Hyperparameters\nBATCH_SIZE = 32\nLEARNING_RATE = 1e-3\nEPOCHS = 200\nNUM_CLASSES = 5\n\n# Train: Heavy augmentation (Rotation, Shift, Zoom/Crop, Flip, Brightness)\ntrain_transforms = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1), # Ensure 1 channel\n    transforms.Resize((224, 224)),\n    transforms.RandomRotation(15),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)), # shift width/height\n    transforms.ColorJitter(brightness=0.2), # brightness 0.8-1.2\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(), # Converts to [0,1]\n])\n\n# Val: No augmentation, just resize and rescale\nval_transforms = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T12:34:54.086188Z","iopub.execute_input":"2025-11-24T12:34:54.086489Z","iopub.status.idle":"2025-11-24T12:34:54.092719Z","shell.execute_reply.started":"2025-11-24T12:34:54.086467Z","shell.execute_reply":"2025-11-24T12:34:54.091843Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_dir = '/kaggle/input/koa-xray/XRAY/train'\nval_dir = '/kaggle/input/koa-xray/XRAY/val'      \n\n# Load Datasets\ntrain_dataset = datasets.ImageFolder(root=train_dir, transform=train_transforms)\nval_dataset = datasets.ImageFolder(root=val_dir, transform=val_transforms)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Validation samples: {len(val_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T12:35:00.753571Z","iopub.execute_input":"2025-11-24T12:35:00.754177Z","iopub.status.idle":"2025-11-24T12:35:04.128312Z","shell.execute_reply.started":"2025-11-24T12:35:00.754151Z","shell.execute_reply":"2025-11-24T12:35:04.127642Z"}},"outputs":[{"name":"stdout","text":"Training samples: 5779\nValidation samples: 856\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Custom CNN Architecture","metadata":{}},{"cell_type":"code","source":"class CustomCNN(nn.Module):\n    def __init__(self, num_classes=5):\n        super(CustomCNN, self).__init__()\n        \n        # Block 1 (Input: 224x224 -> Output: 112x112)\n        self.block1 = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25)\n        )\n        \n        # Block 2 (Input: 112x112 -> Output: 56x56)\n        self.block2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.30)\n        )\n        \n        # Block 3 (Input: 56x56 -> Output: 28x28)\n        self.block3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.35)\n        )\n\n        # Block 4 (Input: 28x28 -> Output: 14x14)\n        self.block4 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.40) # Higher dropout deeper in the network\n        )\n        \n        # Global Average Pooling\n        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        \n        # Classification Head\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x) \n        x = self.global_avg_pool(x)\n        x = self.classifier(x)\n        return x\n\n# Initialize Model\nmodel = CustomCNN(num_classes=NUM_CLASSES).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T20:54:52.424218Z","iopub.execute_input":"2025-11-23T20:54:52.424496Z","iopub.status.idle":"2025-11-23T20:54:52.451552Z","shell.execute_reply.started":"2025-11-23T20:54:52.424476Z","shell.execute_reply":"2025-11-23T20:54:52.450984Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Calculate Class Weights \ny_train_indices = train_dataset.targets\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train_indices), y=y_train_indices)\n\n# Convert to Tensor and move to GPU\nclass_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\nprint(f\"Class Weights: {class_weights}\")\n\n# Loss Function and Optimizer\ncriterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T12:35:59.967459Z","iopub.execute_input":"2025-11-24T12:35:59.968406Z","iopub.status.idle":"2025-11-24T12:35:59.978243Z","shell.execute_reply.started":"2025-11-24T12:35:59.968376Z","shell.execute_reply":"2025-11-24T12:35:59.977436Z"}},"outputs":[{"name":"stdout","text":"Class Weights: [0.5055993  1.10497132 0.76240106 1.52480211 6.68092486]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Training Loop","metadata":{}},{"cell_type":"code","source":"# Define the Training Step\ndef train_one_epoch(model, loader, criterion, optimizer):\n    model.train() # Set model to training mode\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for images, labels in loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Stats\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \n    return running_loss / len(loader), 100 * correct / total\n\n# Define the Validation Step\ndef validate(model, loader, criterion):\n    model.eval() # Set model to evaluation mode (no dropout)\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad(): # Don't calculate gradients for validation\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n    return running_loss / len(loader), 100 * correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T12:36:08.042892Z","iopub.execute_input":"2025-11-24T12:36:08.043340Z","iopub.status.idle":"2025-11-24T12:36:08.050790Z","shell.execute_reply.started":"2025-11-24T12:36:08.043307Z","shell.execute_reply":"2025-11-24T12:36:08.049878Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import copy\n\n# --- CONFIGURATION ---\nPATIENCE = 10          # Stop if no improvement for 10 epochs\nBEST_MODEL_PATH = 'best_model.pth'\n\n# --- EARLY STOPPING CLASS ---\nclass EarlyStopping:\n    def __init__(self, patience=10):\n        self.patience = patience\n        self.counter = 0\n        self.prev_loss = float('inf')\n        self.early_stop = False\n\n    def __call__(self, val_loss):\n        if val_loss > self.prev_loss:\n            self.counter += 1\n        else:\n            self.counter = 0\n        \n        # Update previous loss for next comparison\n        self.prev_loss = val_loss\n\n        if self.counter >= self.patience:\n            self.early_stop = True\n\n\n# Initialize our new specific helper classes\ndivergence_stopper = EarlyStopping(patience=PATIENCE)\nbest_loss_so_far = float('inf') # To track when to save weights\n\nhistory = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n\nprint(f\"Starting Training...\")\n\n# --- 3. TRAINING LOOP ---\nfor epoch in range(EPOCHS):\n    # Train & Validate\n    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n    val_loss, val_acc = validate(model, val_loader, criterion)\n    \n    # Store History\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    \n    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.2f} | \"\n          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}%\")\n    \n    # --- A. SAVE BEST WEIGHTS ---\n    if val_loss < best_loss_so_far:\n        best_loss_so_far = val_loss\n        torch.save(model.state_dict(), BEST_MODEL_PATH)\n        print(f\" New best model found! Saved. (Loss: {val_loss:.4f})\")\n    \n    # --- B. CHECK STOPPING CRITERIA ---\n    divergence_stopper(val_loss)\n    \n    if divergence_stopper.early_stop:\n        print(\"Early stopping triggered!\")\n        break\n\n# --- 4. RELOAD BEST WEIGHTS ---\nif os.path.exists(BEST_MODEL_PATH):\n    model.load_state_dict(torch.load(BEST_MODEL_PATH))\n    print(\"Loaded best model weights.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T21:08:38.362795Z","iopub.execute_input":"2025-11-23T21:08:38.363107Z","iopub.status.idle":"2025-11-23T21:42:10.682884Z","shell.execute_reply.started":"2025-11-23T21:08:38.363080Z","shell.execute_reply":"2025-11-23T21:42:10.682030Z"}},"outputs":[{"name":"stdout","text":"Starting Training...\nEpoch [1/200] Train Loss: 1.6083 Acc: 24.99 | Val Loss: 1.5876 Acc: 27.92%\n New best model found! Saved. (Loss: 1.5876)\nEpoch [2/200] Train Loss: 1.6080 Acc: 38.19 | Val Loss: 1.5761 Acc: 27.92%\n New best model found! Saved. (Loss: 1.5761)\nEpoch [3/200] Train Loss: 1.6084 Acc: 29.36 | Val Loss: 1.5873 Acc: 27.92%\nEpoch [4/200] Train Loss: 1.6072 Acc: 31.18 | Val Loss: 1.5783 Acc: 38.67%\nEpoch [5/200] Train Loss: 1.6076 Acc: 34.90 | Val Loss: 1.5806 Acc: 38.67%\nEpoch [6/200] Train Loss: 1.6075 Acc: 37.74 | Val Loss: 1.5983 Acc: 27.92%\nEpoch [7/200] Train Loss: 1.6077 Acc: 31.35 | Val Loss: 1.5727 Acc: 38.67%\n New best model found! Saved. (Loss: 1.5727)\nEpoch [8/200] Train Loss: 1.6195 Acc: 26.54 | Val Loss: 1.5993 Acc: 27.92%\nEpoch [9/200] Train Loss: 1.6090 Acc: 30.82 | Val Loss: 1.6032 Acc: 17.87%\nEpoch [10/200] Train Loss: 1.6081 Acc: 20.04 | Val Loss: 1.5958 Acc: 19.04%\nEpoch [11/200] Train Loss: 1.6080 Acc: 23.34 | Val Loss: 1.5960 Acc: 24.42%\nEpoch [12/200] Train Loss: 1.6078 Acc: 34.37 | Val Loss: 1.5810 Acc: 38.55%\nEpoch [13/200] Train Loss: 1.6051 Acc: 32.48 | Val Loss: 1.5398 Acc: 38.67%\n New best model found! Saved. (Loss: 1.5398)\nEpoch [14/200] Train Loss: 1.6057 Acc: 30.66 | Val Loss: 1.5669 Acc: 34.70%\nEpoch [15/200] Train Loss: 1.6024 Acc: 29.68 | Val Loss: 1.6097 Acc: 23.48%\nEpoch [16/200] Train Loss: 1.5977 Acc: 30.84 | Val Loss: 1.5892 Acc: 23.83%\nEpoch [17/200] Train Loss: 1.5894 Acc: 31.08 | Val Loss: 1.4837 Acc: 41.24%\n New best model found! Saved. (Loss: 1.4837)\nEpoch [18/200] Train Loss: 1.5314 Acc: 31.22 | Val Loss: 1.3921 Acc: 38.08%\n New best model found! Saved. (Loss: 1.3921)\nEpoch [19/200] Train Loss: 1.4259 Acc: 31.18 | Val Loss: 1.3373 Acc: 42.06%\n New best model found! Saved. (Loss: 1.3373)\nEpoch [20/200] Train Loss: 1.3536 Acc: 34.26 | Val Loss: 1.2831 Acc: 25.00%\n New best model found! Saved. (Loss: 1.2831)\nEpoch [21/200] Train Loss: 1.3140 Acc: 32.39 | Val Loss: 1.3091 Acc: 37.50%\nEpoch [22/200] Train Loss: 1.2703 Acc: 37.83 | Val Loss: 1.2021 Acc: 47.20%\n New best model found! Saved. (Loss: 1.2021)\nEpoch [23/200] Train Loss: 1.2633 Acc: 39.70 | Val Loss: 1.3255 Acc: 33.41%\nEpoch [24/200] Train Loss: 1.2353 Acc: 39.16 | Val Loss: 1.5008 Acc: 24.18%\nEpoch [25/200] Train Loss: 1.2033 Acc: 39.09 | Val Loss: 1.3958 Acc: 26.87%\nEpoch [26/200] Train Loss: 1.1974 Acc: 40.02 | Val Loss: 1.2703 Acc: 35.16%\nEpoch [27/200] Train Loss: 1.1776 Acc: 42.03 | Val Loss: 1.2448 Acc: 30.14%\nEpoch [28/200] Train Loss: 1.1833 Acc: 40.28 | Val Loss: 1.2380 Acc: 42.06%\nEpoch [29/200] Train Loss: 1.1646 Acc: 43.99 | Val Loss: 1.3157 Acc: 35.98%\nEpoch [30/200] Train Loss: 1.1600 Acc: 42.48 | Val Loss: 1.2406 Acc: 36.10%\nEpoch [31/200] Train Loss: 1.1346 Acc: 44.26 | Val Loss: 1.3212 Acc: 30.61%\nEpoch [32/200] Train Loss: 1.1353 Acc: 43.04 | Val Loss: 1.1662 Acc: 50.47%\n New best model found! Saved. (Loss: 1.1662)\nEpoch [33/200] Train Loss: 1.1195 Acc: 44.44 | Val Loss: 1.1954 Acc: 47.78%\nEpoch [34/200] Train Loss: 1.1190 Acc: 45.53 | Val Loss: 1.1570 Acc: 49.53%\n New best model found! Saved. (Loss: 1.1570)\nEpoch [35/200] Train Loss: 1.1281 Acc: 43.69 | Val Loss: 1.2059 Acc: 47.43%\nEpoch [36/200] Train Loss: 1.1078 Acc: 46.18 | Val Loss: 1.1359 Acc: 50.23%\n New best model found! Saved. (Loss: 1.1359)\nEpoch [37/200] Train Loss: 1.1011 Acc: 44.19 | Val Loss: 1.1618 Acc: 50.00%\nEpoch [38/200] Train Loss: 1.1045 Acc: 44.38 | Val Loss: 1.2186 Acc: 44.16%\nEpoch [39/200] Train Loss: 1.0791 Acc: 44.51 | Val Loss: 1.3007 Acc: 38.43%\nEpoch [40/200] Train Loss: 1.0900 Acc: 43.69 | Val Loss: 1.2588 Acc: 30.96%\nEpoch [41/200] Train Loss: 1.0705 Acc: 45.39 | Val Loss: 1.2173 Acc: 47.43%\nEpoch [42/200] Train Loss: 1.0804 Acc: 44.99 | Val Loss: 1.2407 Acc: 39.60%\nEpoch [43/200] Train Loss: 1.0660 Acc: 44.82 | Val Loss: 1.1243 Acc: 51.99%\n New best model found! Saved. (Loss: 1.1243)\nEpoch [44/200] Train Loss: 1.0634 Acc: 45.61 | Val Loss: 1.1202 Acc: 50.47%\n New best model found! Saved. (Loss: 1.1202)\nEpoch [45/200] Train Loss: 1.0665 Acc: 45.77 | Val Loss: 1.3282 Acc: 31.43%\nEpoch [46/200] Train Loss: 1.0485 Acc: 46.77 | Val Loss: 1.1377 Acc: 47.31%\nEpoch [47/200] Train Loss: 1.0449 Acc: 46.77 | Val Loss: 1.1486 Acc: 50.12%\nEpoch [48/200] Train Loss: 1.0494 Acc: 45.94 | Val Loss: 1.2792 Acc: 38.08%\nEpoch [49/200] Train Loss: 1.0317 Acc: 47.31 | Val Loss: 1.2593 Acc: 37.03%\nEpoch [50/200] Train Loss: 1.0195 Acc: 47.43 | Val Loss: 1.0998 Acc: 52.45%\n New best model found! Saved. (Loss: 1.0998)\nEpoch [51/200] Train Loss: 1.0293 Acc: 47.71 | Val Loss: 1.1216 Acc: 47.90%\nEpoch [52/200] Train Loss: 1.0155 Acc: 47.95 | Val Loss: 1.1423 Acc: 43.81%\nEpoch [53/200] Train Loss: 1.0084 Acc: 48.42 | Val Loss: 1.0986 Acc: 49.42%\n New best model found! Saved. (Loss: 1.0986)\nEpoch [54/200] Train Loss: 0.9941 Acc: 48.11 | Val Loss: 1.1322 Acc: 49.42%\nEpoch [55/200] Train Loss: 1.0121 Acc: 48.14 | Val Loss: 1.0938 Acc: 52.57%\n New best model found! Saved. (Loss: 1.0938)\nEpoch [56/200] Train Loss: 0.9874 Acc: 49.13 | Val Loss: 1.1975 Acc: 41.94%\nEpoch [57/200] Train Loss: 0.9939 Acc: 47.55 | Val Loss: 1.0890 Acc: 47.78%\n New best model found! Saved. (Loss: 1.0890)\nEpoch [58/200] Train Loss: 0.9909 Acc: 48.73 | Val Loss: 1.1887 Acc: 45.44%\nEpoch [59/200] Train Loss: 0.9819 Acc: 48.94 | Val Loss: 1.1268 Acc: 51.64%\nEpoch [60/200] Train Loss: 0.9899 Acc: 48.88 | Val Loss: 1.1684 Acc: 44.98%\nEpoch [61/200] Train Loss: 0.9934 Acc: 49.21 | Val Loss: 1.1360 Acc: 49.53%\nEpoch [62/200] Train Loss: 0.9674 Acc: 50.16 | Val Loss: 1.0888 Acc: 48.25%\n New best model found! Saved. (Loss: 1.0888)\nEpoch [63/200] Train Loss: 0.9737 Acc: 49.06 | Val Loss: 1.0817 Acc: 52.22%\n New best model found! Saved. (Loss: 1.0817)\nEpoch [64/200] Train Loss: 0.9815 Acc: 49.56 | Val Loss: 1.0597 Acc: 53.15%\n New best model found! Saved. (Loss: 1.0597)\nEpoch [65/200] Train Loss: 0.9567 Acc: 49.97 | Val Loss: 1.1074 Acc: 43.93%\nEpoch [66/200] Train Loss: 0.9646 Acc: 49.82 | Val Loss: 1.0950 Acc: 51.17%\nEpoch [67/200] Train Loss: 0.9632 Acc: 49.96 | Val Loss: 1.1381 Acc: 40.65%\nEpoch [68/200] Train Loss: 0.9616 Acc: 49.11 | Val Loss: 1.0616 Acc: 53.39%\nEpoch [69/200] Train Loss: 0.9539 Acc: 49.07 | Val Loss: 1.0717 Acc: 51.99%\nEpoch [70/200] Train Loss: 0.9515 Acc: 49.65 | Val Loss: 1.1071 Acc: 51.29%\nEpoch [71/200] Train Loss: 0.9579 Acc: 50.46 | Val Loss: 1.0630 Acc: 55.72%\nEpoch [72/200] Train Loss: 0.9570 Acc: 50.39 | Val Loss: 1.0962 Acc: 51.17%\nEpoch [73/200] Train Loss: 0.9592 Acc: 49.90 | Val Loss: 1.0531 Acc: 53.39%\n New best model found! Saved. (Loss: 1.0531)\nEpoch [74/200] Train Loss: 0.9334 Acc: 50.58 | Val Loss: 1.0823 Acc: 52.69%\nEpoch [75/200] Train Loss: 0.9429 Acc: 49.94 | Val Loss: 1.0558 Acc: 54.09%\nEpoch [76/200] Train Loss: 0.9356 Acc: 51.08 | Val Loss: 1.0564 Acc: 50.82%\nEpoch [77/200] Train Loss: 0.9321 Acc: 51.31 | Val Loss: 1.0243 Acc: 56.43%\n New best model found! Saved. (Loss: 1.0243)\nEpoch [78/200] Train Loss: 0.9325 Acc: 50.49 | Val Loss: 1.0289 Acc: 53.27%\nEpoch [79/200] Train Loss: 0.9244 Acc: 50.67 | Val Loss: 1.1523 Acc: 47.55%\nEpoch [80/200] Train Loss: 0.9377 Acc: 51.34 | Val Loss: 1.0943 Acc: 51.40%\nEpoch [81/200] Train Loss: 0.9180 Acc: 52.31 | Val Loss: 1.0500 Acc: 53.74%\nEpoch [82/200] Train Loss: 0.9267 Acc: 52.36 | Val Loss: 1.0407 Acc: 54.79%\nEpoch [83/200] Train Loss: 0.9183 Acc: 51.89 | Val Loss: 1.0264 Acc: 55.26%\nEpoch [84/200] Train Loss: 0.9187 Acc: 51.81 | Val Loss: 1.0092 Acc: 55.49%\n New best model found! Saved. (Loss: 1.0092)\nEpoch [85/200] Train Loss: 0.9001 Acc: 52.14 | Val Loss: 1.0261 Acc: 55.26%\nEpoch [86/200] Train Loss: 0.9141 Acc: 51.44 | Val Loss: 1.0738 Acc: 52.45%\nEpoch [87/200] Train Loss: 0.9151 Acc: 51.95 | Val Loss: 0.9938 Acc: 57.01%\n New best model found! Saved. (Loss: 0.9938)\nEpoch [88/200] Train Loss: 0.9105 Acc: 52.78 | Val Loss: 1.0263 Acc: 54.56%\nEpoch [89/200] Train Loss: 0.8987 Acc: 52.79 | Val Loss: 1.0581 Acc: 52.80%\nEpoch [90/200] Train Loss: 0.8983 Acc: 53.71 | Val Loss: 1.0418 Acc: 55.14%\nEpoch [91/200] Train Loss: 0.8951 Acc: 53.69 | Val Loss: 0.9791 Acc: 57.71%\n New best model found! Saved. (Loss: 0.9791)\nEpoch [92/200] Train Loss: 0.9041 Acc: 52.34 | Val Loss: 1.0056 Acc: 58.06%\nEpoch [93/200] Train Loss: 0.9029 Acc: 53.69 | Val Loss: 1.0326 Acc: 53.50%\nEpoch [94/200] Train Loss: 0.8876 Acc: 53.16 | Val Loss: 1.0468 Acc: 53.39%\nEpoch [95/200] Train Loss: 0.8804 Acc: 52.95 | Val Loss: 1.0126 Acc: 53.97%\nEpoch [96/200] Train Loss: 0.8931 Acc: 53.28 | Val Loss: 0.9966 Acc: 56.43%\nEpoch [97/200] Train Loss: 0.8661 Acc: 53.33 | Val Loss: 1.0070 Acc: 56.31%\nEpoch [98/200] Train Loss: 0.8760 Acc: 54.37 | Val Loss: 1.0827 Acc: 52.22%\nEpoch [99/200] Train Loss: 0.8827 Acc: 53.68 | Val Loss: 1.0296 Acc: 48.25%\nEpoch [100/200] Train Loss: 0.8739 Acc: 52.28 | Val Loss: 0.9741 Acc: 58.64%\n New best model found! Saved. (Loss: 0.9741)\nEpoch [101/200] Train Loss: 0.8720 Acc: 54.27 | Val Loss: 0.9622 Acc: 58.06%\n New best model found! Saved. (Loss: 0.9622)\nEpoch [102/200] Train Loss: 0.8614 Acc: 54.53 | Val Loss: 0.9799 Acc: 59.35%\nEpoch [103/200] Train Loss: 0.8702 Acc: 53.99 | Val Loss: 0.9788 Acc: 55.49%\nEpoch [104/200] Train Loss: 0.8733 Acc: 53.49 | Val Loss: 1.0169 Acc: 55.14%\nEpoch [105/200] Train Loss: 0.8484 Acc: 55.70 | Val Loss: 0.9821 Acc: 55.26%\nEpoch [106/200] Train Loss: 0.8715 Acc: 53.37 | Val Loss: 1.0315 Acc: 53.39%\nEpoch [107/200] Train Loss: 0.8553 Acc: 55.84 | Val Loss: 0.9885 Acc: 55.61%\nEpoch [108/200] Train Loss: 0.8663 Acc: 55.30 | Val Loss: 0.9747 Acc: 57.59%\nEpoch [109/200] Train Loss: 0.8594 Acc: 55.17 | Val Loss: 0.9788 Acc: 56.19%\nEpoch [110/200] Train Loss: 0.8637 Acc: 54.58 | Val Loss: 0.9581 Acc: 58.88%\n New best model found! Saved. (Loss: 0.9581)\nEpoch [111/200] Train Loss: 0.8520 Acc: 54.80 | Val Loss: 0.9850 Acc: 55.96%\nEpoch [112/200] Train Loss: 0.8428 Acc: 55.46 | Val Loss: 1.0104 Acc: 55.02%\nEpoch [113/200] Train Loss: 0.8561 Acc: 55.15 | Val Loss: 0.9863 Acc: 55.37%\nEpoch [114/200] Train Loss: 0.8428 Acc: 55.74 | Val Loss: 0.9861 Acc: 56.31%\nEpoch [115/200] Train Loss: 0.8204 Acc: 55.86 | Val Loss: 1.0339 Acc: 51.87%\nEpoch [116/200] Train Loss: 0.8357 Acc: 55.13 | Val Loss: 0.9718 Acc: 60.05%\nEpoch [117/200] Train Loss: 0.8294 Acc: 55.23 | Val Loss: 0.9638 Acc: 54.32%\nEpoch [118/200] Train Loss: 0.8360 Acc: 55.20 | Val Loss: 0.9784 Acc: 54.32%\nEpoch [119/200] Train Loss: 0.8254 Acc: 56.55 | Val Loss: 0.9704 Acc: 59.00%\nEpoch [120/200] Train Loss: 0.8328 Acc: 55.65 | Val Loss: 0.9756 Acc: 56.54%\nEpoch [121/200] Train Loss: 0.8254 Acc: 55.23 | Val Loss: 0.9640 Acc: 55.49%\nEpoch [122/200] Train Loss: 0.8359 Acc: 55.55 | Val Loss: 0.9512 Acc: 56.43%\n New best model found! Saved. (Loss: 0.9512)\nEpoch [123/200] Train Loss: 0.8304 Acc: 56.29 | Val Loss: 1.0165 Acc: 53.50%\nEpoch [124/200] Train Loss: 0.8329 Acc: 55.44 | Val Loss: 0.9492 Acc: 57.36%\n New best model found! Saved. (Loss: 0.9492)\nEpoch [125/200] Train Loss: 0.8288 Acc: 55.56 | Val Loss: 0.9361 Acc: 57.83%\n New best model found! Saved. (Loss: 0.9361)\nEpoch [126/200] Train Loss: 0.8204 Acc: 56.39 | Val Loss: 0.9702 Acc: 53.86%\nEpoch [127/200] Train Loss: 0.8387 Acc: 55.72 | Val Loss: 1.0048 Acc: 55.26%\nEpoch [128/200] Train Loss: 0.8233 Acc: 57.05 | Val Loss: 0.9648 Acc: 54.91%\nEpoch [129/200] Train Loss: 0.8255 Acc: 56.53 | Val Loss: 0.9605 Acc: 59.23%\nEpoch [130/200] Train Loss: 0.8150 Acc: 56.51 | Val Loss: 0.9436 Acc: 59.00%\nEpoch [131/200] Train Loss: 0.8157 Acc: 56.15 | Val Loss: 0.9406 Acc: 57.71%\nEpoch [132/200] Train Loss: 0.8049 Acc: 56.36 | Val Loss: 0.9641 Acc: 57.71%\nEpoch [133/200] Train Loss: 0.8043 Acc: 55.58 | Val Loss: 0.9407 Acc: 60.05%\nEpoch [134/200] Train Loss: 0.8079 Acc: 57.29 | Val Loss: 1.1659 Acc: 46.73%\nEpoch [135/200] Train Loss: 0.8312 Acc: 55.03 | Val Loss: 0.9695 Acc: 53.15%\nEpoch [136/200] Train Loss: 0.7991 Acc: 57.19 | Val Loss: 0.9595 Acc: 57.59%\nEpoch [137/200] Train Loss: 0.8373 Acc: 54.99 | Val Loss: 0.9457 Acc: 57.83%\nEpoch [138/200] Train Loss: 0.8082 Acc: 56.13 | Val Loss: 0.9875 Acc: 54.79%\nEpoch [139/200] Train Loss: 0.8256 Acc: 55.22 | Val Loss: 0.9567 Acc: 57.01%\nEpoch [140/200] Train Loss: 0.8237 Acc: 56.26 | Val Loss: 1.0027 Acc: 51.40%\nEpoch [141/200] Train Loss: 0.7936 Acc: 57.33 | Val Loss: 0.9554 Acc: 56.07%\nEpoch [142/200] Train Loss: 0.8139 Acc: 55.84 | Val Loss: 0.9415 Acc: 58.29%\nEpoch [143/200] Train Loss: 0.8007 Acc: 57.36 | Val Loss: 0.9706 Acc: 55.96%\nEpoch [144/200] Train Loss: 0.7848 Acc: 56.39 | Val Loss: 0.9520 Acc: 57.01%\nEpoch [145/200] Train Loss: 0.7888 Acc: 57.48 | Val Loss: 0.9415 Acc: 57.59%\nEpoch [146/200] Train Loss: 0.7945 Acc: 56.01 | Val Loss: 0.9590 Acc: 57.59%\nEpoch [147/200] Train Loss: 0.8272 Acc: 56.50 | Val Loss: 0.9931 Acc: 56.19%\nEpoch [148/200] Train Loss: 0.8105 Acc: 55.91 | Val Loss: 1.0372 Acc: 50.35%\nEpoch [149/200] Train Loss: 0.7929 Acc: 56.50 | Val Loss: 0.9439 Acc: 59.58%\nEpoch [150/200] Train Loss: 0.7819 Acc: 58.94 | Val Loss: 0.9697 Acc: 55.61%\nEpoch [151/200] Train Loss: 0.7717 Acc: 57.80 | Val Loss: 0.9322 Acc: 60.05%\n New best model found! Saved. (Loss: 0.9322)\nEpoch [152/200] Train Loss: 0.7988 Acc: 56.53 | Val Loss: 0.9694 Acc: 54.67%\nEpoch [153/200] Train Loss: 0.8060 Acc: 57.61 | Val Loss: 0.9336 Acc: 58.53%\nEpoch [154/200] Train Loss: 0.8028 Acc: 57.81 | Val Loss: 0.9761 Acc: 54.67%\nEpoch [155/200] Train Loss: 0.7830 Acc: 57.22 | Val Loss: 0.9499 Acc: 56.31%\nEpoch [156/200] Train Loss: 0.7974 Acc: 56.83 | Val Loss: 0.9406 Acc: 55.96%\nEpoch [157/200] Train Loss: 0.7737 Acc: 58.40 | Val Loss: 0.9572 Acc: 57.13%\nEpoch [158/200] Train Loss: 0.7715 Acc: 58.90 | Val Loss: 0.9973 Acc: 54.91%\nEpoch [159/200] Train Loss: 0.7987 Acc: 58.40 | Val Loss: 0.9693 Acc: 59.58%\nEpoch [160/200] Train Loss: 0.7799 Acc: 58.44 | Val Loss: 0.9014 Acc: 63.08%\n New best model found! Saved. (Loss: 0.9014)\nEpoch [161/200] Train Loss: 0.7740 Acc: 57.83 | Val Loss: 0.9713 Acc: 55.14%\nEpoch [162/200] Train Loss: 0.7835 Acc: 57.59 | Val Loss: 0.9792 Acc: 54.44%\nEpoch [163/200] Train Loss: 0.7820 Acc: 57.71 | Val Loss: 0.9562 Acc: 58.06%\nEpoch [164/200] Train Loss: 0.7694 Acc: 58.25 | Val Loss: 0.9394 Acc: 59.70%\nEpoch [165/200] Train Loss: 0.7767 Acc: 57.62 | Val Loss: 0.9313 Acc: 60.86%\nEpoch [166/200] Train Loss: 0.7823 Acc: 57.81 | Val Loss: 0.9531 Acc: 60.63%\nEpoch [167/200] Train Loss: 0.7687 Acc: 59.34 | Val Loss: 0.9341 Acc: 59.46%\nEpoch [168/200] Train Loss: 0.7628 Acc: 58.71 | Val Loss: 0.9158 Acc: 59.11%\nEpoch [169/200] Train Loss: 0.7737 Acc: 57.24 | Val Loss: 0.9697 Acc: 57.36%\nEpoch [170/200] Train Loss: 0.7825 Acc: 57.67 | Val Loss: 0.9492 Acc: 56.78%\nEpoch [171/200] Train Loss: 0.7574 Acc: 59.53 | Val Loss: 0.9528 Acc: 58.88%\nEpoch [172/200] Train Loss: 0.7699 Acc: 58.66 | Val Loss: 0.9209 Acc: 59.81%\nEpoch [173/200] Train Loss: 0.7730 Acc: 58.38 | Val Loss: 0.9285 Acc: 57.94%\nEpoch [174/200] Train Loss: 0.7584 Acc: 58.89 | Val Loss: 0.9256 Acc: 59.81%\nEpoch [175/200] Train Loss: 0.7627 Acc: 58.44 | Val Loss: 0.9308 Acc: 58.41%\nEpoch [176/200] Train Loss: 0.7401 Acc: 58.94 | Val Loss: 0.9616 Acc: 54.67%\nEpoch [177/200] Train Loss: 0.7500 Acc: 59.09 | Val Loss: 0.9223 Acc: 60.05%\nEpoch [178/200] Train Loss: 0.7768 Acc: 58.16 | Val Loss: 0.9134 Acc: 60.16%\nEpoch [179/200] Train Loss: 0.7425 Acc: 59.27 | Val Loss: 0.9772 Acc: 56.66%\nEpoch [180/200] Train Loss: 0.7611 Acc: 59.01 | Val Loss: 1.0215 Acc: 53.15%\nEpoch [181/200] Train Loss: 0.7633 Acc: 59.35 | Val Loss: 0.9616 Acc: 58.18%\nEpoch [182/200] Train Loss: 0.7492 Acc: 58.78 | Val Loss: 0.9525 Acc: 59.58%\nEpoch [183/200] Train Loss: 0.7542 Acc: 58.23 | Val Loss: 0.9626 Acc: 57.83%\nEpoch [184/200] Train Loss: 0.7623 Acc: 58.85 | Val Loss: 1.0217 Acc: 55.02%\nEpoch [185/200] Train Loss: 0.7522 Acc: 58.92 | Val Loss: 0.9226 Acc: 58.41%\nEpoch [186/200] Train Loss: 0.7566 Acc: 59.01 | Val Loss: 0.9481 Acc: 57.59%\nEpoch [187/200] Train Loss: 0.7507 Acc: 59.75 | Val Loss: 1.0181 Acc: 54.44%\nEpoch [188/200] Train Loss: 0.7504 Acc: 58.99 | Val Loss: 1.0727 Acc: 47.66%\nEpoch [189/200] Train Loss: 0.7540 Acc: 58.80 | Val Loss: 0.9216 Acc: 59.23%\nEpoch [190/200] Train Loss: 0.7425 Acc: 59.13 | Val Loss: 0.9201 Acc: 60.51%\nEpoch [191/200] Train Loss: 0.7497 Acc: 59.35 | Val Loss: 0.9441 Acc: 59.00%\nEpoch [192/200] Train Loss: 0.7456 Acc: 60.06 | Val Loss: 0.9746 Acc: 53.86%\nEpoch [193/200] Train Loss: 0.7688 Acc: 58.45 | Val Loss: 0.8995 Acc: 61.33%\n New best model found! Saved. (Loss: 0.8995)\nEpoch [194/200] Train Loss: 0.7397 Acc: 59.92 | Val Loss: 0.9164 Acc: 57.13%\nEpoch [195/200] Train Loss: 0.7385 Acc: 58.71 | Val Loss: 0.9055 Acc: 61.33%\nEpoch [196/200] Train Loss: 0.7520 Acc: 59.18 | Val Loss: 0.9395 Acc: 59.11%\nEpoch [197/200] Train Loss: 0.7450 Acc: 59.34 | Val Loss: 0.9496 Acc: 56.89%\nEpoch [198/200] Train Loss: 0.7355 Acc: 59.99 | Val Loss: 1.0057 Acc: 55.49%\nEpoch [199/200] Train Loss: 0.7453 Acc: 60.06 | Val Loss: 0.9055 Acc: 61.57%\nEpoch [200/200] Train Loss: 0.7440 Acc: 59.42 | Val Loss: 0.9689 Acc: 54.21%\nLoaded best model weights.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"model = CustomCNN(num_classes=5).to(device) \ncheckpoint_path = '../input/my-koa-model/best_model.pth' \n\nif os.path.exists(checkpoint_path):\n    print(f\"Loading weights from {checkpoint_path}...\")\n    model.load_state_dict(torch.load(checkpoint_path))\n    print(\"Weights loaded successfully!\")\nelse:\n    print(\"No checkpoint found.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T18:57:17.642154Z","iopub.status.idle":"2025-11-23T18:57:17.642416Z","shell.execute_reply.started":"2025-11-23T18:57:17.642302Z","shell.execute_reply":"2025-11-23T18:57:17.642322Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Evaluation","metadata":{}},{"cell_type":"code","source":"# 1. Get all predictions\nall_preds = []\nall_labels = []\n\nmodel.eval()\nwith torch.no_grad():\n    for images, labels in val_loader:\n        images = images.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        \n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.numpy())\n\n# 2. Classification Report\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(all_labels, all_preds, digits=4))\n\n# 3. Confusion Matrix Plot\ndef plot_confusion_matrix(cm, classes):\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(\"Confusion Matrix\")\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n\ncm = confusion_matrix(all_labels, all_preds)\nplot_confusion_matrix(cm, classes=['0', '1', '2', '3', '4'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T23:17:24.671166Z","iopub.execute_input":"2025-11-23T23:17:24.671843Z","iopub.status.idle":"2025-11-23T23:17:26.999511Z","shell.execute_reply.started":"2025-11-23T23:17:24.671809Z","shell.execute_reply":"2025-11-23T23:17:26.998432Z"}},"outputs":[{"name":"stdout","text":"\nClassification Report:\n\n              precision    recall  f1-score   support\n\n           0     0.6990    0.8066    0.7489       331\n           1     0.3245    0.3203    0.3224       153\n           2     0.7273    0.6025    0.6590       239\n           3     0.7579    0.6792    0.7164       106\n           4     0.7667    0.8519    0.8070        27\n\n    accuracy                         0.6484       856\n   macro avg     0.6551    0.6521    0.6508       856\nweighted avg     0.6494    0.6484    0.6454       856\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":34},{"cell_type":"markdown","source":"# AlexNet\nSlightly modified for training and evaluation","metadata":{}},{"cell_type":"code","source":"class AlexNet_KOA(nn.Module):\n    def __init__(self, num_classes=5):\n        super(AlexNet_KOA, self).__init__()\n        \n        # Feature Extractor\n        self.features = nn.Sequential(\n            # Conv 1\n            nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            # Conv 2\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            # Conv 3\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            # Conv 4\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            # Conv 5\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        \n        # Global Average Pooling (Modern adjustment to reduce parameters)\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        \n        # Classifier Head\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.5),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# 2. Initialize\nmodel = AlexNet_KOA(num_classes=5).to(device)\n\n# 3. Optimizer\noptimizer = optim.Adam(model.parameters(), lr=1e-4) \n\n# 4. Loss Function (Keep using your weighted loss!)\ncriterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n\nprint(\"AlexNet initialized for KOA benchmarking!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T21:02:27.370858Z","iopub.execute_input":"2025-11-23T21:02:27.371387Z","iopub.status.idle":"2025-11-23T21:02:27.911968Z","shell.execute_reply.started":"2025-11-23T21:02:27.371354Z","shell.execute_reply":"2025-11-23T21:02:27.911290Z"}},"outputs":[{"name":"stdout","text":"AlexNet initialized for KOA benchmarking!\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# DenseNet\npre-trained on ImageNet, will be trained on our dataset","metadata":{}},{"cell_type":"code","source":"# Run this BEFORE the DenseNet block\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.Grayscale(num_output_channels=3), # <--- MUST BE 3 FOR DENSENET\n    transforms.RandomRotation(15),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n    transforms.ColorJitter(brightness=0.2),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.Grayscale(num_output_channels=3), # <--- MUST BE 3\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Re-load data with new transforms\ntrain_dataset = datasets.ImageFolder(root=train_dir, transform=train_transforms)\nval_dataset = datasets.ImageFolder(root=val_dir, transform=val_transforms)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T21:51:21.926479Z","iopub.execute_input":"2025-11-23T21:51:21.926803Z","iopub.status.idle":"2025-11-23T21:51:25.836538Z","shell.execute_reply.started":"2025-11-23T21:51:21.926777Z","shell.execute_reply":"2025-11-23T21:51:25.835944Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\n\n# --- CONFIGURATION ---\n# If you hit Out Of Memory (OOM), change BATCH_SIZE to 16\nEPOCHS = 100\nPATIENCE = 10\nDENSENET_PATH = 'densenet121_best.pth'\n\n# 1. DEFINE DENSENET BUILDER\ndef get_densenet(num_classes=5):\n    # Load Pretrained DenseNet121\n    # weights=\"DEFAULT\" downloads the best available ImageNet weights\n    model = models.densenet121(weights=\"DEFAULT\")\n    \n    # UNFREEZE: We want to fine-tune the whole model for X-rays\n    # (No need to set requires_grad=False)\n    \n    # Replace the Classifier\n    # DenseNet's classifier is a linear layer named 'classifier'\n    num_ftrs = model.classifier.in_features\n    \n    model.classifier = nn.Sequential(\n        nn.Linear(num_ftrs, 512),\n        nn.ReLU(),\n        nn.Dropout(0.5),\n        nn.Linear(512, num_classes)\n    )\n    \n    return model\n\n# 2. INITIALIZE\nprint(\"Initializing DenseNet121...\")\nmodel = get_densenet(num_classes=5).to(device)\n\n# 3. OPTIMIZER & LOSS\n# DenseNet is deep and complex. \n# We use a lower Learning Rate (1e-4) to carefully fine-tune the pretrained weights.\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n\n# 4. HELPERS\ndivergence_stopper = EarlyStopping(patience=PATIENCE)\nbest_loss_so_far = float('inf')\nhistory_densenet = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n\n# 5. TRAINING LOOP\nprint(f\"Starting DenseNet Training (Saving to: {DENSENET_PATH})...\")\n\nfor epoch in range(EPOCHS):\n    # Train & Validate\n    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n    val_loss, val_acc = validate(model, val_loader, criterion)\n    \n    # Log\n    history_densenet['train_loss'].append(train_loss)\n    history_densenet['train_acc'].append(train_acc)\n    history_densenet['val_loss'].append(val_loss)\n    history_densenet['val_acc'].append(val_acc)\n    \n    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | \"\n          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}%\")\n    \n    # --- SAVE BEST ---\n    if val_loss < best_loss_so_far:\n        best_loss_so_far = val_loss\n        torch.save(model.state_dict(), DENSENET_PATH)\n        print(f\" New best found! Saved. (Loss: {val_loss:.4f})\")\n    \n    # --- STOPPING ---\n    divergence_stopper(val_loss)\n    if divergence_stopper.early_stop:\n        print(\"Early stopping triggered!\")\n        break\n\n# 6. RELOAD BEST\nif os.path.exists(DENSENET_PATH):\n    model.load_state_dict(torch.load(DENSENET_PATH))\n    print(\"Loaded best DenseNet weights.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T22:11:17.171023Z","iopub.execute_input":"2025-11-23T22:11:17.171418Z","iopub.status.idle":"2025-11-23T23:11:35.279882Z","shell.execute_reply.started":"2025-11-23T22:11:17.171393Z","shell.execute_reply":"2025-11-23T23:11:35.278891Z"}},"outputs":[{"name":"stdout","text":"Initializing DenseNet121...\nStarting DenseNet Training (Saving to: densenet121_best.pth)...\nEpoch [1/100] Train Loss: 1.3210 Acc: 35.65% | Val Loss: 1.0789 Acc: 56.19%\n New best found! Saved. (Loss: 1.0789)\nEpoch [2/100] Train Loss: 0.9299 Acc: 53.87% | Val Loss: 0.9617 Acc: 58.29%\n New best found! Saved. (Loss: 0.9617)\nEpoch [3/100] Train Loss: 0.8704 Acc: 56.60% | Val Loss: 0.9338 Acc: 57.71%\n New best found! Saved. (Loss: 0.9338)\nEpoch [4/100] Train Loss: 0.8128 Acc: 59.70% | Val Loss: 1.0059 Acc: 55.84%\nEpoch [5/100] Train Loss: 0.7577 Acc: 61.29% | Val Loss: 0.8908 Acc: 61.10%\n New best found! Saved. (Loss: 0.8908)\nEpoch [6/100] Train Loss: 0.7824 Acc: 61.69% | Val Loss: 0.8879 Acc: 61.68%\n New best found! Saved. (Loss: 0.8879)\nEpoch [7/100] Train Loss: 0.7175 Acc: 62.78% | Val Loss: 0.9082 Acc: 58.06%\nEpoch [8/100] Train Loss: 0.7046 Acc: 65.27% | Val Loss: 0.8931 Acc: 60.05%\nEpoch [9/100] Train Loss: 0.6691 Acc: 65.88% | Val Loss: 0.8284 Acc: 63.79%\n New best found! Saved. (Loss: 0.8284)\nEpoch [10/100] Train Loss: 0.6695 Acc: 66.26% | Val Loss: 0.8507 Acc: 59.93%\nEpoch [11/100] Train Loss: 0.6360 Acc: 66.78% | Val Loss: 0.8189 Acc: 63.55%\n New best found! Saved. (Loss: 0.8189)\nEpoch [12/100] Train Loss: 0.6225 Acc: 68.61% | Val Loss: 0.8556 Acc: 61.45%\nEpoch [13/100] Train Loss: 0.6141 Acc: 68.56% | Val Loss: 0.8163 Acc: 64.02%\n New best found! Saved. (Loss: 0.8163)\nEpoch [14/100] Train Loss: 0.5916 Acc: 69.44% | Val Loss: 0.7921 Acc: 64.84%\n New best found! Saved. (Loss: 0.7921)\nEpoch [15/100] Train Loss: 0.5811 Acc: 69.58% | Val Loss: 0.8359 Acc: 63.08%\nEpoch [16/100] Train Loss: 0.5587 Acc: 71.74% | Val Loss: 0.8283 Acc: 64.14%\nEpoch [17/100] Train Loss: 0.5519 Acc: 72.04% | Val Loss: 0.8877 Acc: 64.84%\nEpoch [18/100] Train Loss: 0.5500 Acc: 71.52% | Val Loss: 0.9018 Acc: 64.14%\nEpoch [19/100] Train Loss: 0.5264 Acc: 73.63% | Val Loss: 0.8791 Acc: 65.30%\nEpoch [20/100] Train Loss: 0.5028 Acc: 73.42% | Val Loss: 0.9870 Acc: 61.33%\nEpoch [21/100] Train Loss: 0.4876 Acc: 74.74% | Val Loss: 0.9243 Acc: 64.60%\nEpoch [22/100] Train Loss: 0.4678 Acc: 76.00% | Val Loss: 0.8802 Acc: 65.07%\nEpoch [23/100] Train Loss: 0.4364 Acc: 77.59% | Val Loss: 1.0903 Acc: 59.46%\nEpoch [24/100] Train Loss: 0.4545 Acc: 77.70% | Val Loss: 0.9787 Acc: 64.84%\nEpoch [25/100] Train Loss: 0.4453 Acc: 78.25% | Val Loss: 0.9831 Acc: 60.16%\nEpoch [26/100] Train Loss: 0.4109 Acc: 80.20% | Val Loss: 1.0356 Acc: 62.27%\nEpoch [27/100] Train Loss: 0.3738 Acc: 80.79% | Val Loss: 1.2024 Acc: 58.18%\nEpoch [28/100] Train Loss: 0.3767 Acc: 81.80% | Val Loss: 1.0212 Acc: 60.86%\nEpoch [29/100] Train Loss: 0.3521 Acc: 81.95% | Val Loss: 1.0110 Acc: 64.25%\nEpoch [30/100] Train Loss: 0.3271 Acc: 84.79% | Val Loss: 1.1721 Acc: 63.20%\nEpoch [31/100] Train Loss: 0.3225 Acc: 83.68% | Val Loss: 1.2078 Acc: 58.41%\nEpoch [32/100] Train Loss: 0.3199 Acc: 84.86% | Val Loss: 1.1079 Acc: 62.85%\nEpoch [33/100] Train Loss: 0.2900 Acc: 85.79% | Val Loss: 1.2748 Acc: 61.68%\nEpoch [34/100] Train Loss: 0.3100 Acc: 85.21% | Val Loss: 1.1916 Acc: 60.75%\nEpoch [35/100] Train Loss: 0.2608 Acc: 87.73% | Val Loss: 1.3239 Acc: 62.15%\nEpoch [36/100] Train Loss: 0.2611 Acc: 87.35% | Val Loss: 1.2202 Acc: 61.57%\nEpoch [37/100] Train Loss: 0.2531 Acc: 88.29% | Val Loss: 1.4056 Acc: 64.49%\nEpoch [38/100] Train Loss: 0.2282 Acc: 88.96% | Val Loss: 1.4202 Acc: 59.70%\nEpoch [39/100] Train Loss: 0.2402 Acc: 88.42% | Val Loss: 1.2539 Acc: 65.19%\nEpoch [40/100] Train Loss: 0.2021 Acc: 89.91% | Val Loss: 1.2925 Acc: 64.84%\nEpoch [41/100] Train Loss: 0.2348 Acc: 88.96% | Val Loss: 1.2760 Acc: 65.07%\nEpoch [42/100] Train Loss: 0.1964 Acc: 91.16% | Val Loss: 1.4132 Acc: 62.50%\nEpoch [43/100] Train Loss: 0.1787 Acc: 90.97% | Val Loss: 1.4080 Acc: 60.75%\nEpoch [44/100] Train Loss: 0.1749 Acc: 91.82% | Val Loss: 1.5299 Acc: 65.65%\nEpoch [45/100] Train Loss: 0.1770 Acc: 91.90% | Val Loss: 1.4291 Acc: 64.37%\nEpoch [46/100] Train Loss: 0.1819 Acc: 91.59% | Val Loss: 1.7727 Acc: 55.02%\nEpoch [47/100] Train Loss: 0.1913 Acc: 91.76% | Val Loss: 1.4122 Acc: 64.25%\nEpoch [48/100] Train Loss: 0.1677 Acc: 92.04% | Val Loss: 1.5163 Acc: 63.43%\nEpoch [49/100] Train Loss: 0.1651 Acc: 92.59% | Val Loss: 1.6518 Acc: 60.75%\nEpoch [50/100] Train Loss: 0.1549 Acc: 93.53% | Val Loss: 1.7154 Acc: 62.38%\nEpoch [51/100] Train Loss: 0.1383 Acc: 93.41% | Val Loss: 1.6325 Acc: 58.76%\nEpoch [52/100] Train Loss: 0.1458 Acc: 93.49% | Val Loss: 1.6892 Acc: 65.65%\nEpoch [53/100] Train Loss: 0.1397 Acc: 93.16% | Val Loss: 1.5625 Acc: 62.85%\nEpoch [54/100] Train Loss: 0.1249 Acc: 94.20% | Val Loss: 1.5763 Acc: 63.43%\nEpoch [55/100] Train Loss: 0.1819 Acc: 92.37% | Val Loss: 1.3558 Acc: 65.42%\nEpoch [56/100] Train Loss: 0.1479 Acc: 94.05% | Val Loss: 1.6933 Acc: 62.73%\nEpoch [57/100] Train Loss: 0.1214 Acc: 94.38% | Val Loss: 1.6202 Acc: 64.14%\nEpoch [58/100] Train Loss: 0.1229 Acc: 94.43% | Val Loss: 1.6483 Acc: 63.79%\nEpoch [59/100] Train Loss: 0.1240 Acc: 94.72% | Val Loss: 1.7647 Acc: 64.72%\nEpoch [60/100] Train Loss: 0.1008 Acc: 95.03% | Val Loss: 1.7311 Acc: 65.54%\nEpoch [61/100] Train Loss: 0.1104 Acc: 95.24% | Val Loss: 1.7679 Acc: 64.95%\nEpoch [62/100] Train Loss: 0.1213 Acc: 94.41% | Val Loss: 1.7681 Acc: 64.72%\nEpoch [63/100] Train Loss: 0.1166 Acc: 94.79% | Val Loss: 1.6042 Acc: 64.60%\nEpoch [64/100] Train Loss: 0.0826 Acc: 96.42% | Val Loss: 1.8706 Acc: 64.37%\nEpoch [65/100] Train Loss: 0.0918 Acc: 95.71% | Val Loss: 1.8425 Acc: 61.57%\nEpoch [66/100] Train Loss: 0.0871 Acc: 96.21% | Val Loss: 1.8105 Acc: 61.92%\nEpoch [67/100] Train Loss: 0.1009 Acc: 95.69% | Val Loss: 1.7279 Acc: 61.80%\nEpoch [68/100] Train Loss: 0.0956 Acc: 95.33% | Val Loss: 2.0556 Acc: 63.90%\nEpoch [69/100] Train Loss: 0.1113 Acc: 94.88% | Val Loss: 1.9684 Acc: 62.38%\nEpoch [70/100] Train Loss: 0.1108 Acc: 95.74% | Val Loss: 1.8951 Acc: 59.58%\nEpoch [71/100] Train Loss: 0.0758 Acc: 96.61% | Val Loss: 1.8307 Acc: 63.55%\nEpoch [72/100] Train Loss: 0.0914 Acc: 95.64% | Val Loss: 1.9345 Acc: 60.28%\nEpoch [73/100] Train Loss: 0.0906 Acc: 95.92% | Val Loss: 1.8399 Acc: 62.03%\nEpoch [74/100] Train Loss: 0.0798 Acc: 96.23% | Val Loss: 1.7562 Acc: 63.08%\nEpoch [75/100] Train Loss: 0.0924 Acc: 95.81% | Val Loss: 1.8493 Acc: 61.45%\nEpoch [76/100] Train Loss: 0.0979 Acc: 95.67% | Val Loss: 1.8188 Acc: 64.49%\nEpoch [77/100] Train Loss: 0.1396 Acc: 94.77% | Val Loss: 1.9492 Acc: 63.08%\nEpoch [78/100] Train Loss: 0.0676 Acc: 97.06% | Val Loss: 1.9922 Acc: 62.38%\nEpoch [79/100] Train Loss: 0.0752 Acc: 96.49% | Val Loss: 2.0896 Acc: 64.60%\nEpoch [80/100] Train Loss: 0.0853 Acc: 96.25% | Val Loss: 1.8812 Acc: 64.02%\nEpoch [81/100] Train Loss: 0.0673 Acc: 96.92% | Val Loss: 1.9924 Acc: 63.32%\nEpoch [82/100] Train Loss: 0.0615 Acc: 96.99% | Val Loss: 2.1212 Acc: 60.98%\nEpoch [83/100] Train Loss: 0.0713 Acc: 96.80% | Val Loss: 1.9815 Acc: 59.23%\nEpoch [84/100] Train Loss: 0.1079 Acc: 95.71% | Val Loss: 1.9571 Acc: 57.59%\nEpoch [85/100] Train Loss: 0.0680 Acc: 97.01% | Val Loss: 1.8916 Acc: 64.14%\nEpoch [86/100] Train Loss: 0.1052 Acc: 95.78% | Val Loss: 1.8242 Acc: 63.79%\nEpoch [87/100] Train Loss: 0.0835 Acc: 96.35% | Val Loss: 1.9484 Acc: 64.72%\nEpoch [88/100] Train Loss: 0.0792 Acc: 96.49% | Val Loss: 1.9151 Acc: 64.49%\nEpoch [89/100] Train Loss: 0.0612 Acc: 97.21% | Val Loss: 2.0404 Acc: 64.14%\nEpoch [90/100] Train Loss: 0.0673 Acc: 96.97% | Val Loss: 1.9781 Acc: 64.95%\nEpoch [91/100] Train Loss: 0.0546 Acc: 97.46% | Val Loss: 1.9893 Acc: 64.49%\nEpoch [92/100] Train Loss: 0.0619 Acc: 97.18% | Val Loss: 2.1734 Acc: 59.23%\nEpoch [93/100] Train Loss: 0.0574 Acc: 97.56% | Val Loss: 2.1793 Acc: 60.75%\nEpoch [94/100] Train Loss: 0.0562 Acc: 97.53% | Val Loss: 2.0539 Acc: 64.14%\nEpoch [95/100] Train Loss: 0.0648 Acc: 97.02% | Val Loss: 2.0308 Acc: 61.57%\nEpoch [96/100] Train Loss: 0.0708 Acc: 97.04% | Val Loss: 1.9142 Acc: 59.93%\nEpoch [97/100] Train Loss: 0.1085 Acc: 95.95% | Val Loss: 1.9215 Acc: 62.50%\nEpoch [98/100] Train Loss: 0.0737 Acc: 96.80% | Val Loss: 2.0179 Acc: 59.70%\nEpoch [99/100] Train Loss: 0.0629 Acc: 97.58% | Val Loss: 1.9013 Acc: 59.81%\nEpoch [100/100] Train Loss: 0.0830 Acc: 96.45% | Val Loss: 1.9124 Acc: 64.14%\nLoaded best DenseNet weights.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"# TorchXRayVision\nDenseNet + ResNet trained specifically on 200K+ XRays (mostly chest)","metadata":{}},{"cell_type":"code","source":"# 1. Install (Run once)\n!pip install torchxrayvision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T12:30:09.095282Z","iopub.execute_input":"2025-11-24T12:30:09.095549Z","iopub.status.idle":"2025-11-24T12:31:20.897416Z","shell.execute_reply.started":"2025-11-24T12:30:09.095532Z","shell.execute_reply":"2025-11-24T12:31:20.896567Z"}},"outputs":[{"name":"stdout","text":"Collecting torchxrayvision\n  Downloading torchxrayvision-1.4.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: torch>=1 in /usr/local/lib/python3.11/dist-packages (from torchxrayvision) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.11/dist-packages (from torchxrayvision) (0.21.0+cu124)\nRequirement already satisfied: scikit-image>=0.16 in /usr/local/lib/python3.11/dist-packages (from torchxrayvision) (0.25.2)\nRequirement already satisfied: tqdm>=4 in /usr/local/lib/python3.11/dist-packages (from torchxrayvision) (4.67.1)\nRequirement already satisfied: numpy>=1 in /usr/local/lib/python3.11/dist-packages (from torchxrayvision) (1.26.4)\nRequirement already satisfied: pandas>=1 in /usr/local/lib/python3.11/dist-packages (from torchxrayvision) (2.2.3)\nRequirement already satisfied: requests>=1 in /usr/local/lib/python3.11/dist-packages (from torchxrayvision) (2.32.5)\nRequirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchxrayvision) (11.3.0)\nRequirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from torchxrayvision) (2.37.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1->torchxrayvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1->torchxrayvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1->torchxrayvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1->torchxrayvision) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1->torchxrayvision) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1->torchxrayvision) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1->torchxrayvision) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1->torchxrayvision) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1->torchxrayvision) (2025.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=1->torchxrayvision) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=1->torchxrayvision) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=1->torchxrayvision) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=1->torchxrayvision) (2025.10.5)\nRequirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16->torchxrayvision) (1.15.3)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16->torchxrayvision) (3.5)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16->torchxrayvision) (2025.6.11)\nRequirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16->torchxrayvision) (25.0)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16->torchxrayvision) (0.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1->torchxrayvision) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1->torchxrayvision) (4.15.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1->torchxrayvision) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1->torchxrayvision) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1->torchxrayvision)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1->torchxrayvision)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1->torchxrayvision)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1->torchxrayvision)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1->torchxrayvision)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1->torchxrayvision)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1->torchxrayvision)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1->torchxrayvision)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1->torchxrayvision)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1->torchxrayvision) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1->torchxrayvision) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1->torchxrayvision) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1->torchxrayvision)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1->torchxrayvision) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1->torchxrayvision) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1->torchxrayvision) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1->torchxrayvision) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1->torchxrayvision) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1->torchxrayvision) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1->torchxrayvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1->torchxrayvision) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1->torchxrayvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1->torchxrayvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1->torchxrayvision) (2024.2.0)\nDownloading torchxrayvision-1.4.0-py3-none-any.whl (29.0 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchxrayvision\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchxrayvision-1.4.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torchxrayvision as xrv\n\n\ndef get_xray_model(num_classes=5):\n    # Load model pre-trained on ALL major chest datasets\n    model = xrv.models.DenseNet(weights=\"densenet121-res224-all\")\n    \n    # This prevents the model from trying to normalize outputs based on old classes\n    model.op_threshs = None\n    \n    # Replace Classifier\n    # Note: xrv models output a specific dictionary, but we can hack the classifier\n    num_ftrs = model.classifier.in_features\n    \n    model.classifier = nn.Sequential(\n        nn.Linear(num_ftrs, 512),\n        nn.ReLU(),\n        nn.Dropout(0.5),\n        nn.Linear(512, num_classes)\n    )\n    return model\n\n# 2. TRANSFORM ADAPATION (CRITICAL)\n# XRV expects 1-channel input, but distinct normalization\nxray_transforms = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    # DO NOT USE ImageNet Normalize ([0.485...])\n    # XRV handles raw pixel values better, or we can just leave it as 0-1\n])\n\n# 3. Initialize\nmodel = get_xray_model(num_classes=5)\nmodel = model.to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T12:40:41.473671Z","iopub.execute_input":"2025-11-24T12:40:41.474419Z","iopub.status.idle":"2025-11-24T12:40:41.795505Z","shell.execute_reply.started":"2025-11-24T12:40:41.474385Z","shell.execute_reply":"2025-11-24T12:40:41.794819Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import copy\n\n# --- CONFIGURATION ---\nPATIENCE = 10          # Stop if no improvement for 10 epochs\nBEST_MODEL_PATH = 'best_model_txv.pth'\n\n# --- EARLY STOPPING CLASS ---\nclass EarlyStopping:\n    def __init__(self, patience=10):\n        self.patience = patience\n        self.counter = 0\n        self.prev_loss = float('inf')\n        self.early_stop = False\n\n    def __call__(self, val_loss):\n        if val_loss > self.prev_loss:\n            self.counter += 1\n        else:\n            self.counter = 0\n        \n        # Update previous loss for next comparison\n        self.prev_loss = val_loss\n\n        if self.counter >= self.patience:\n            self.early_stop = True\n\n\n# Initialize our new specific helper classes\ndivergence_stopper = EarlyStopping(patience=PATIENCE)\nbest_loss_so_far = float('inf') # To track when to save weights\n\nhistory = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n\nprint(f\"Starting Training...\")\n\n# --- 3. TRAINING LOOP ---\nfor epoch in range(EPOCHS):\n    # Train & Validate\n    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n    val_loss, val_acc = validate(model, val_loader, criterion)\n    \n    # Store History\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    \n    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.2f} | \"\n          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}%\")\n    \n    # --- A. SAVE BEST WEIGHTS ---\n    if val_loss < best_loss_so_far:\n        best_loss_so_far = val_loss\n        torch.save(model.state_dict(), BEST_MODEL_PATH)\n        print(f\" New best model found! Saved. (Loss: {val_loss:.4f})\")\n    \n    # --- B. CHECK STOPPING CRITERIA ---\n    divergence_stopper(val_loss)\n    \n    if divergence_stopper.early_stop:\n        print(\"Early stopping triggered!\")\n        break\n\n# --- 4. RELOAD BEST WEIGHTS ---\nif os.path.exists(BEST_MODEL_PATH):\n    model.load_state_dict(torch.load(BEST_MODEL_PATH))\n    print(\"Loaded best model weights.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T12:40:47.212623Z","iopub.execute_input":"2025-11-24T12:40:47.213321Z","execution_failed":"2025-11-24T15:11:51.451Z"}},"outputs":[{"name":"stdout","text":"Starting Training...\nEpoch [1/200] Train Loss: 1.5834 Acc: 32.88 | Val Loss: 1.4339 Acc: 42.99%\n New best model found! Saved. (Loss: 1.4339)\nEpoch [2/200] Train Loss: 1.3762 Acc: 39.92 | Val Loss: 1.2103 Acc: 45.91%\n New best model found! Saved. (Loss: 1.2103)\nEpoch [3/200] Train Loss: 1.1730 Acc: 45.09 | Val Loss: 1.1915 Acc: 43.69%\n New best model found! Saved. (Loss: 1.1915)\nEpoch [4/200] Train Loss: 1.0899 Acc: 46.96 | Val Loss: 1.1366 Acc: 51.40%\n New best model found! Saved. (Loss: 1.1366)\nEpoch [5/200] Train Loss: 1.0132 Acc: 49.70 | Val Loss: 1.3318 Acc: 40.77%\nEpoch [6/200] Train Loss: 0.9542 Acc: 51.62 | Val Loss: 1.3347 Acc: 43.46%\nEpoch [7/200] Train Loss: 0.9501 Acc: 53.11 | Val Loss: 1.0522 Acc: 53.62%\n New best model found! Saved. (Loss: 1.0522)\nEpoch [8/200] Train Loss: 0.9173 Acc: 53.43 | Val Loss: 1.0198 Acc: 56.89%\n New best model found! Saved. (Loss: 1.0198)\nEpoch [9/200] Train Loss: 0.8953 Acc: 54.68 | Val Loss: 0.9991 Acc: 57.71%\n New best model found! Saved. (Loss: 0.9991)\nEpoch [10/200] Train Loss: 0.8832 Acc: 55.03 | Val Loss: 1.0439 Acc: 53.86%\nEpoch [11/200] Train Loss: 0.8790 Acc: 56.03 | Val Loss: 1.0383 Acc: 56.07%\nEpoch [12/200] Train Loss: 0.8738 Acc: 56.26 | Val Loss: 1.0334 Acc: 56.54%\nEpoch [13/200] Train Loss: 0.8510 Acc: 57.03 | Val Loss: 1.0430 Acc: 57.01%\nEpoch [14/200] Train Loss: 0.8508 Acc: 56.39 | Val Loss: 0.9800 Acc: 58.06%\n New best model found! Saved. (Loss: 0.9800)\nEpoch [15/200] Train Loss: 0.8289 Acc: 57.50 | Val Loss: 1.0761 Acc: 54.67%\nEpoch [16/200] Train Loss: 0.8328 Acc: 58.45 | Val Loss: 1.0067 Acc: 55.02%\nEpoch [17/200] Train Loss: 0.8208 Acc: 58.61 | Val Loss: 0.9803 Acc: 57.71%\nEpoch [18/200] Train Loss: 0.7973 Acc: 59.44 | Val Loss: 0.9561 Acc: 60.05%\n New best model found! Saved. (Loss: 0.9561)\nEpoch [19/200] Train Loss: 0.8114 Acc: 58.97 | Val Loss: 0.9507 Acc: 59.23%\n New best model found! Saved. (Loss: 0.9507)\nEpoch [20/200] Train Loss: 0.8035 Acc: 58.50 | Val Loss: 0.9547 Acc: 60.51%\nEpoch [21/200] Train Loss: 0.7963 Acc: 59.82 | Val Loss: 0.9931 Acc: 57.83%\nEpoch [22/200] Train Loss: 0.7802 Acc: 59.84 | Val Loss: 1.0095 Acc: 58.88%\nEpoch [23/200] Train Loss: 0.7655 Acc: 61.33 | Val Loss: 0.9524 Acc: 60.16%\nEpoch [24/200] Train Loss: 0.7796 Acc: 60.49 | Val Loss: 0.9397 Acc: 59.11%\n New best model found! Saved. (Loss: 0.9397)\nEpoch [25/200] Train Loss: 0.7691 Acc: 60.30 | Val Loss: 0.9665 Acc: 59.81%\nEpoch [26/200] Train Loss: 0.7537 Acc: 61.05 | Val Loss: 0.9366 Acc: 60.28%\n New best model found! Saved. (Loss: 0.9366)\nEpoch [27/200] Train Loss: 0.7644 Acc: 61.08 | Val Loss: 0.9660 Acc: 60.86%\nEpoch [28/200] Train Loss: 0.7537 Acc: 60.65 | Val Loss: 1.0179 Acc: 56.43%\nEpoch [29/200] Train Loss: 0.7430 Acc: 61.26 | Val Loss: 0.9464 Acc: 60.28%\nEpoch [30/200] Train Loss: 0.7438 Acc: 62.40 | Val Loss: 0.9813 Acc: 60.86%\nEpoch [31/200] Train Loss: 0.7402 Acc: 61.74 | Val Loss: 0.9616 Acc: 59.46%\nEpoch [32/200] Train Loss: 0.7239 Acc: 63.14 | Val Loss: 0.9636 Acc: 57.36%\nEpoch [33/200] Train Loss: 0.7180 Acc: 62.87 | Val Loss: 0.9569 Acc: 57.83%\nEpoch [34/200] Train Loss: 0.7209 Acc: 62.43 | Val Loss: 0.9164 Acc: 60.98%\n New best model found! Saved. (Loss: 0.9164)\nEpoch [35/200] Train Loss: 0.7289 Acc: 62.35 | Val Loss: 0.9461 Acc: 58.29%\nEpoch [36/200] Train Loss: 0.7145 Acc: 62.74 | Val Loss: 0.9424 Acc: 60.86%\nEpoch [37/200] Train Loss: 0.7017 Acc: 63.51 | Val Loss: 0.9345 Acc: 58.76%\nEpoch [38/200] Train Loss: 0.7102 Acc: 63.61 | Val Loss: 0.9145 Acc: 59.23%\n New best model found! Saved. (Loss: 0.9145)\nEpoch [39/200] Train Loss: 0.7120 Acc: 63.28 | Val Loss: 0.9467 Acc: 59.93%\nEpoch [40/200] Train Loss: 0.7116 Acc: 63.07 | Val Loss: 0.9258 Acc: 59.58%\nEpoch [41/200] Train Loss: 0.7006 Acc: 64.39 | Val Loss: 0.9278 Acc: 59.93%\nEpoch [42/200] Train Loss: 0.6897 Acc: 63.40 | Val Loss: 0.9260 Acc: 60.75%\nEpoch [43/200] Train Loss: 0.6787 Acc: 64.51 | Val Loss: 0.9943 Acc: 58.76%\nEpoch [44/200] Train Loss: 0.6880 Acc: 64.22 | Val Loss: 0.9655 Acc: 60.05%\nEpoch [45/200] Train Loss: 0.6819 Acc: 63.73 | Val Loss: 0.9048 Acc: 62.03%\n New best model found! Saved. (Loss: 0.9048)\nEpoch [46/200] Train Loss: 0.6757 Acc: 65.20 | Val Loss: 0.9534 Acc: 60.28%\nEpoch [47/200] Train Loss: 0.6691 Acc: 65.24 | Val Loss: 0.9148 Acc: 59.58%\nEpoch [48/200] Train Loss: 0.6616 Acc: 65.55 | Val Loss: 0.9256 Acc: 60.63%\nEpoch [49/200] Train Loss: 0.6731 Acc: 64.75 | Val Loss: 0.8973 Acc: 61.45%\n New best model found! Saved. (Loss: 0.8973)\nEpoch [50/200] Train Loss: 0.6535 Acc: 65.70 | Val Loss: 0.9161 Acc: 61.21%\nEpoch [51/200] Train Loss: 0.6561 Acc: 65.39 | Val Loss: 0.8874 Acc: 63.20%\n New best model found! Saved. (Loss: 0.8874)\nEpoch [52/200] Train Loss: 0.6630 Acc: 65.65 | Val Loss: 0.9159 Acc: 61.92%\nEpoch [53/200] Train Loss: 0.6420 Acc: 66.66 | Val Loss: 0.9021 Acc: 60.40%\nEpoch [54/200] Train Loss: 0.6454 Acc: 66.03 | Val Loss: 0.9708 Acc: 59.23%\nEpoch [55/200] Train Loss: 0.6555 Acc: 65.06 | Val Loss: 0.8946 Acc: 61.80%\nEpoch [56/200] Train Loss: 0.6357 Acc: 66.46 | Val Loss: 0.9360 Acc: 63.67%\nEpoch [57/200] Train Loss: 0.6368 Acc: 66.00 | Val Loss: 0.9147 Acc: 63.43%\nEpoch [58/200] Train Loss: 0.6451 Acc: 66.71 | Val Loss: 0.9192 Acc: 60.98%\nEpoch [59/200] Train Loss: 0.6310 Acc: 66.19 | Val Loss: 0.9148 Acc: 62.62%\nEpoch [60/200] Train Loss: 0.6312 Acc: 65.70 | Val Loss: 0.9157 Acc: 60.40%\nEpoch [61/200] Train Loss: 0.6276 Acc: 66.67 | Val Loss: 0.9522 Acc: 59.58%\nEpoch [62/200] Train Loss: 0.6098 Acc: 67.00 | Val Loss: 0.9044 Acc: 62.03%\nEpoch [63/200] Train Loss: 0.6222 Acc: 67.83 | Val Loss: 0.9319 Acc: 61.10%\nEpoch [64/200] Train Loss: 0.6131 Acc: 67.49 | Val Loss: 0.9583 Acc: 63.08%\nEpoch [65/200] Train Loss: 0.6140 Acc: 67.14 | Val Loss: 0.9810 Acc: 59.70%\nEpoch [66/200] Train Loss: 0.6296 Acc: 66.78 | Val Loss: 0.8942 Acc: 61.68%\nEpoch [67/200] Train Loss: 0.6129 Acc: 67.54 | Val Loss: 0.8934 Acc: 63.20%\nEpoch [68/200] Train Loss: 0.5962 Acc: 68.40 | Val Loss: 0.9014 Acc: 63.20%\nEpoch [69/200] Train Loss: 0.6172 Acc: 67.43 | Val Loss: 0.9135 Acc: 60.98%\nEpoch [70/200] Train Loss: 0.5923 Acc: 67.85 | Val Loss: 0.9159 Acc: 58.88%\nEpoch [71/200] Train Loss: 0.5989 Acc: 67.97 | Val Loss: 0.9522 Acc: 60.63%\nEpoch [72/200] Train Loss: 0.5941 Acc: 68.13 | Val Loss: 0.9247 Acc: 58.76%\nEpoch [73/200] Train Loss: 0.5880 Acc: 68.18 | Val Loss: 0.9345 Acc: 61.57%\nEpoch [74/200] Train Loss: 0.5985 Acc: 68.06 | Val Loss: 0.9151 Acc: 62.15%\nEpoch [75/200] Train Loss: 0.5786 Acc: 69.18 | Val Loss: 0.9188 Acc: 62.73%\nEpoch [76/200] Train Loss: 0.5809 Acc: 69.16 | Val Loss: 0.9460 Acc: 61.45%\nEpoch [77/200] Train Loss: 0.5747 Acc: 69.35 | Val Loss: 0.9353 Acc: 61.68%\nEpoch [78/200] Train Loss: 0.5761 Acc: 69.04 | Val Loss: 0.9313 Acc: 61.68%\nEpoch [79/200] Train Loss: 0.5752 Acc: 69.72 | Val Loss: 0.9383 Acc: 62.03%\nEpoch [80/200] Train Loss: 0.5649 Acc: 69.56 | Val Loss: 0.9597 Acc: 60.05%\nEpoch [81/200] Train Loss: 0.5681 Acc: 69.51 | Val Loss: 0.9338 Acc: 57.24%\nEpoch [82/200] Train Loss: 0.5789 Acc: 69.48 | Val Loss: 0.9628 Acc: 60.40%\nEpoch [83/200] Train Loss: 0.5735 Acc: 70.01 | Val Loss: 0.9272 Acc: 61.57%\nEpoch [84/200] Train Loss: 0.5476 Acc: 70.38 | Val Loss: 0.9769 Acc: 61.21%\nEpoch [85/200] Train Loss: 0.5725 Acc: 69.46 | Val Loss: 0.9755 Acc: 59.11%\nEpoch [86/200] Train Loss: 0.5557 Acc: 70.79 | Val Loss: 0.9446 Acc: 59.35%\nEpoch [87/200] Train Loss: 0.5663 Acc: 69.79 | Val Loss: 0.9633 Acc: 60.16%\nEpoch [88/200] Train Loss: 0.5560 Acc: 70.41 | Val Loss: 0.9630 Acc: 60.75%\nEpoch [89/200] Train Loss: 0.5494 Acc: 70.93 | Val Loss: 0.9794 Acc: 59.93%\nEpoch [90/200] Train Loss: 0.5449 Acc: 70.46 | Val Loss: 0.9750 Acc: 63.67%\nEpoch [91/200] Train Loss: 0.5392 Acc: 70.34 | Val Loss: 0.9624 Acc: 59.93%\nEpoch [92/200] Train Loss: 0.5454 Acc: 70.41 | Val Loss: 0.9582 Acc: 60.40%\nEpoch [93/200] Train Loss: 0.5345 Acc: 70.69 | Val Loss: 0.9651 Acc: 61.10%\nEpoch [94/200] Train Loss: 0.5439 Acc: 71.17 | Val Loss: 0.9406 Acc: 62.15%\nEpoch [95/200] Train Loss: 0.5333 Acc: 71.29 | Val Loss: 0.9662 Acc: 63.67%\nEpoch [96/200] Train Loss: 0.5415 Acc: 70.44 | Val Loss: 0.9432 Acc: 62.27%\nEpoch [97/200] Train Loss: 0.5301 Acc: 71.59 | Val Loss: 0.9550 Acc: 61.92%\nEpoch [98/200] Train Loss: 0.5264 Acc: 71.29 | Val Loss: 1.0264 Acc: 61.45%\nEpoch [99/200] Train Loss: 0.5311 Acc: 72.50 | Val Loss: 1.0389 Acc: 60.51%\nEpoch [100/200] Train Loss: 0.5212 Acc: 71.81 | Val Loss: 0.9564 Acc: 61.45%\nEpoch [101/200] Train Loss: 0.5182 Acc: 71.74 | Val Loss: 1.0415 Acc: 60.98%\nEpoch [102/200] Train Loss: 0.5139 Acc: 72.54 | Val Loss: 0.9824 Acc: 62.15%\nEpoch [103/200] Train Loss: 0.5075 Acc: 72.19 | Val Loss: 0.9633 Acc: 62.03%\nEpoch [104/200] Train Loss: 0.4993 Acc: 72.76 | Val Loss: 1.0348 Acc: 59.81%\nEpoch [105/200] Train Loss: 0.5164 Acc: 72.68 | Val Loss: 1.0400 Acc: 62.38%\nEpoch [106/200] Train Loss: 0.5093 Acc: 72.88 | Val Loss: 1.0304 Acc: 60.98%\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# 1. Get all predictions\nall_preds = []\nall_labels = []\n\nmodel.eval()\nwith torch.no_grad():\n    for images, labels in val_loader:\n        images = images.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        \n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.numpy())\n\n# 2. Classification Report\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(all_labels, all_preds, digits=4))\n\n# 3. Confusion Matrix Plot\ndef plot_confusion_matrix(cm, classes):\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(\"Confusion Matrix\")\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n\ncm = confusion_matrix(all_labels, all_preds)\nplot_confusion_matrix(cm, classes=['0', '1', '2', '3', '4'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T15:12:05.577469Z","iopub.execute_input":"2025-11-24T15:12:05.578155Z","iopub.status.idle":"2025-11-24T15:12:05.644521Z","shell.execute_reply.started":"2025-11-24T15:12:05.578126Z","shell.execute_reply":"2025-11-24T15:12:05.643603Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/3065830370.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}],"execution_count":1}]}